For every function:
In the parameters, have an overwrite parameter.

if overwrite = 1, go ahead and replace any existing output file
if overwrite = 0, don't overwrite existing file.

---------

Find Stimulus Location
2 types of Inputs:
 Type 1 (parameters structure to describe stimulus):
  - size of cross (almost always 11 px) (make this a parameter)
  - thickness (always 1 px)
 Type 2 (image file representing the stimulus)
  - white cross on black background image, for example (made in paint)
  - convert to a 0 to 255 array to cross correlate.

result: get how much stimulus moves from one frame to the other
(cross correlating stimulus matrix to each frame)
(just run the existing function with enhancements turned off)
have verbosity options available
  level 1: final result graph
  level 2: frame by frame correlation map with the arrow pointing to peak

why: we want to know where the stimulus is and then we can remove it
so that later we can get the eye movement and not the stimulus location

save result as a mat file.
naming convention:
input: myvideo_dwt.avi
output: myvideo_dwt_stimlocs.mat (save input parameters, image path, delete everything else)

add-on: also save the mean and sd of each frame into that mat file.

-------

Remove the stimulus
Use output of previous step.

take the location from mat file
replace in the frames of the dwt version with noise
noise need the mean and sd of the noise
randn(xsize, ysize) ==> mean = 0, sd = 1.
for mean = 60, sd = 30, you multiply by 30, add 60.

(we have this from the save mat file, the mean and sd)
so for each frame, we need the mean and sd of each frame (when cross still in)
then use that to make random noise.
put that noise rectangle, centered on top of the old cross.

by default, use the parameters from mat file.
but also allow for overriding: height and width of noise rectangle.

output: myvideo_dwt_nostim.avi

-----------

Detect blinks and bad frames
When this happens, entire frame is all black or close to that.

the mean luminence will be generally steady, but at the blink, there will be a dip over the course of about 6 frames (a blink is 200 milliseconds)

essentially we want to get "derivatives" and just mark the super negative ones as a blink
  - start with mean of each frame (from previous mat output)
  - get the diff() of these (you'll end up with a size one smaller)
  - calculate med_diff_mu = the median of the diff_mu's.
  - calculate std_med = sqrt( med(diff_mu ^2) - (med(diff_mu))^2 )    (note that the first term is squaring each value of diff_mu, before taking median)
  - calculate threshold = med_diff_mu - 2 * std_med    (this 2 constant should be a parameter, defaulting to 2)
  - define badFrames = [0, diff(mu)] < threshold
  - then call conv() to grab the frames before and after the badFrames already identified
      - conv() with array [1 1 1], badFrames, there is also an option to get the result of conv() to be the same size as badFrames.
      - conv() will have 0,1,2,3 values. change this back to a logical array. array  = array >= 1 or something like that.
  - repeat all of this for the sd of each frame
  - logically OR the two resulting logical arrays from mean and sd frames together
  - use find() to convert this logical array to the indices of badFrames.


output: ..._blinkframes.mat  mat file of bad frames (where the blink ocurred)

--------------

Gamma Correction

gamma_exponent = 0.6
imadjust(2darray, 'gamma', gamma_exponent)  (or something like that)

output: myvideo_dwt_nostim_gamscaled.avi

----------

Bandpass filtering

output: myvideo_dwt_nostim_gamscaled_bandfilt.avi

frequency domain vs. amplitude (power)

take your signal
pass through fourier transform

pass through low pass filter (LPF) (sets one part to 1, the rest to 0)
do the inverse fourier transform to get the original signal but low pass filtered
see pics

Signal - signal low pass filtered => signal high pass filtered

spacial domain and frequency domain instead of time and frequency.


we will do everything in spacial domain without going into frequency domain.
you have your image and a small kernel in it (3x3, 5x5, etc. very small)
calculate the average within the kernel and set the center to that value to blur the whole image. (blurring)

x x x
x 0 x
x x x

1 -1 1
1 -1 1
1 -1 1   use this as kernel to enhance vertical lines (similar for horizontal) (subtract middle column, add sides, then average)

built-in matlab: imgausfilt(2darray, sigmasdofgaussianinpixels)      (kernel size is automatically selected)
sigma = 0 means no weight means same as input image is the result.
sigma = infinity = equal weights

use sigma = 3 for now.
imgausfilt is your low pass filter (no fourier necessary)

.3 .5 .3
.5 1  .5
.3 .5 .3

output: ...bandfilt.avi

----------

Make Coarse Reference Frame

Adaptive search essentially.
Takes videoName, takes params structure
Params has which frame as reference frame. By default, 15? (grab the middle frame of total frames). Takes scaling factor to shrink. Start with 3 ish. 
Read, shrink, store. Cross correlate using the middle frame. 
Get frame shifts, multiply by the downscale factor. (now you have how much shift there is between frames). 
CoarseRef

output: myvideo_dwt_nostim_gamscaled_bandfilt_coarseref.mat

Save the reference frame as a 2D matrix. Call it coarseReferenceFrame.
Also save the frame shifts (x and y shifts between each frame and the initial reference frame).

---------

Convert eye positions from pixels to degrees. (512 px x 512 px to 10 deg x 10 deg)
Apply saccade detection algorithm to get saccades and drifts.
(These are arrays of structures that contain info on different time segments of the eye traces)

This info for saccades and drifts are both:
 - onsetTime (time stamp of the start of that event)
 - offsetTime (time stamp of end of that event)
 - xStart, yStart, xEnd, yEnd
 - duration (which is offsetTime - onsetTime)
 - amplitude.x (absolute value of xStart - xEnd)
 - amplitude.y
 - amplitude.vector (which is pythagorean theorem of xStart to xEnd and yStart to yEnd length)
 - direction (which is atand or atand2d (whichever gives 360 deg range), delta y / delta x) (initial point to end point)
 - position.x (just cut out the original data from eye traces)
 - position.y
 - time (just cut out the original data)
 - velocity.x  (which is [0; diff(position) ./ diff(time)]) (another way is to do: vel_n = (x_(n+1) - x_(n-1)) / 2 delta t) (implement them and make them user-selectable)
 - velocity.y
 - meanVelocity.x
 - meanVelocity.y
 - peakVelocity.x (which is the max velocity)
 - peakVelocity.y
 - acceleration.x (use the same method to take a second derivative)
 - acceleration.y

Saccade Detection
Similar to blink detection, except we don't convolute.
Instead, we use a high multiplier to get the peak, and then a smaller multiplier to make sure we grab the entire peak.
Our multiplier is 6 in this case (typically) this gets the peak. then lower the multiplier to essentially convolute nearby stuff (see photo on phone).
Threshold = multiplier * standarddev +- median. (Make sure to grab both upwards and downwards peaks).
Do this for both x and y, and take logical OR.
Input:
path to final.mat file
parametersStructure.thresholdValue (default 6)
parametersStructure.secondaryThresholdValue (default 2)
parametersStructure.stitchCriteria (default 15 ms) (this will be used to lump together 2 microsaccades that are too close because that was due to wobble in a second pass) (end of one to beginning of next)
parametersStructure.minAmplitude (default 0.1 deg) (in a third pass, remove artifactial saccades that have amplitude less than this)
parametersStructure.maxDuration (default 100 ms) (in a fourth pass, discard if they are longer than 100 ms)
parametersStructure.detectionMethod (default 2) (median-based detection is 2, which we will use by default) (another method, method 1, is just a hard-coded threshold)
parametersStructure.hardVelocityThreshold (default 35 deg/sec) (for hard-coded threshold purposes)
parametersStructure.velocityMethod (1 is most basic, 2 is the one we will use) (default 2)
Output:
append whatever you like to full input name

----

write permisison issues with diary.
(issue arises when you try to run a parallel trim two times for example.)
